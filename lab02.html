<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Experimentation | Hadley Arcemont</title>
    <link rel="stylesheet" href="css/styles.css">
</head>
<body>
    <header>
        <nav>
            <a href="index.html">Home</a>
            <a href="lab02.html" class="active">Lab 2: AI Experimentation</a>
            <a href="lab02.html">Lab 2: AI Experimentation</a>
            <!-- Add more lab links as the semester progresses -->
        </nav>
        <h1>AI Experimentation</h1>
    </header>

    <main>
        <section>
            <h2>What I Tried</h2>
            <p>In this lab, I experimented with two AI tools, ChatGPT and Perplexity, with the goal of learning how these systems behave when I push them outside normal productivity use. Instead of trying to get the “best” output, I treated each tool like an object to study. I ran the same unconventional prompts across both tools to probe their assumptions, find their edges, stretch them creatively, interrogate how their interfaces shape what users ask for, and document dead ends.</p>
        </section>

        <section>
            <h2>What Happened</h2>
            <p>The first experiment tested assumptions by forcing each tool to ask six clarifying questions before “helping.” Both tools complied easily, which suggests that when instructions are explicit, they can perform structured, assignment aligned behavior reliably. The more interesting part was how quickly each tool inferred a story about me. ChatGPT steered toward an evidence based, analytical reflection and assumed I already had a portfolio workflow. Perplexity asked similarly useful questions but sounded more like a writing coach, emphasizing tone and the impression on the reader. In both cases, the tools moved fast from a vague request to an implied model of what success should look like, revealing that AI does not just respond to user goals, it actively frames them.<p>
                
            
            <p>Experiment B was designed to find the edge where a tool must either admit failure or produce something misleading. I gave both tools a prompt with conflicting constraints: provide direct quotes from credible sources and APA citations, but do not use or access any sources. ChatGPT acknowledged the task was impossible under those rules and treated fabricating quotes as an ethical problem. It then produced the closest alternative that met measurable formatting constraints while explicitly dropping the quotes and citations. Perplexity did not acknowledge the contradiction. Instead, it generated quotes and a references list anyway, which looked academically legitimate on the surface but raised a major verification issue. The response provided no clear proof that those quotes were real or accurately attributed. This experiment exposed a key difference in failure modes. One tool disclosed the boundary, while the other performed credibility in a way that could be copied into a portfolio without detection.<p>

                 <!-- Add screenshots or examples -->
            <figure>
                <img src="images/B-chatgpt.png" alt="Experiment B: ChatGPT explains the prompt is impossible due to conflicting source requirements and does not invent quotes or citations." width="85%">
                <figcaption>ChatGPT identifies that the prompt is internally contradictory because it requires direct quotes and APA citations while also forbidding access to sources, and it refuses to invent evidence. It then offers a best effort alternative that follows the formatting rules it can meet while clearly stating what it cannot do ethically.</figcaption>
            </figure>
            <figure>
                <img src="images/B-perplexity.png" alt="Experiment B: Perplexity provides quotes and a references list despite the no-sources constraint." width="85%">
                <figcaption>Perplexity responds as if the prompt is solvable and generates two quotes plus a references list, even though the prompt prohibits using or accessing sources. The screenshot highlights how a response can look academically credible while still being difficult to verify.</figcaption>
            </figure>
            
            </p>Experiment C stretched the tools creatively by asking them to redesign their own interfaces to reduce overtrust. ChatGPT’s suggestions centered on making hidden uncertainty visible through features like an assumptions panel, confidence labels, and a use with care summary. Perplexity’s redesign leaned heavily into verification mechanics, proposing verify this buttons, an evidence mix panel, and a citation quality meter. Nearly every trust reducing feature added friction or complexity, highlighting a tension between convenience and responsible use.</p>

            <figure>
                <img src="images/C-perplexity1.png" alt="Experiment C: Perplexity proposes interface changes like verify-this buttons, an evidence mix panel, and a citation quality meter." width="85%">
                <figcaption>In experiment C, Perplexity proposes interface features meant to reduce overtrust, such as “verify this” buttons, an evidence strength panel, and a citation quality meter. The screenshot shows how the tool frames trust as something managed through citation and verification workflows rather than just better writing.</figcaption>
            </figure>
            
            </p>Experiment D treated the interface itself as a cultural artifact. Both tools argued that the single large text box and chat flow encourage broad prompts and one shot answers, and that linear conversations make it hard to track assumptions, compare drafts, or audit earlier decisions. ChatGPT focused on how a lack of default citations normalizes unsourced claims and how final answer behavior is rewarded. Perplexity focused more on how the interface encourages users to ask for the answer rather than naming perspective, and how it nudges people into over scoped, prediction style questions.</p>

            <!-- Add screenshots or examples, I added my screenshots under the experiment they were a part of to make my portfolio and storytelling flow better. -->
            
        </section>

        <section>
            <h2>What I Learned</h2>
            <p>Across these experiments, I learned that these tools are optimized for plausible helpfulness, not guaranteed truth. The most important difference was how they handled uncertainty and constraints. ChatGPT was more likely to name a contradiction and avoid inventing evidence, while Perplexity was more likely to produce a polished, source shaped response even when sourcing was not actually permitted. Both tools also showed that interface design matters as much as model behavior, because it encourages speed, broad questions, and acceptance of fluent output as final. In DCDA work, that means the human responsibilities are defining the question, insisting on perspective and context, verifying claims, and documenting a traceable research process.</p>
        </section>

        <section>
            <h2>Looking Forward</h2>
            <p>Going forward, I plan to use AI tools as amplifiers of my thinking rather than shortcuts that replace it. In DCDA projects, I can use them to brainstorm research questions, translate stakeholder goals into measurable variables, draft interview or scraping plans, and generate multiple interpretations of the same pattern so I can test them against actual data. I can also use them to prototype code comments, explain errors in plain language, and draft documentation that makes my workflow legible to others. I will avoid using AI as an authority for factual claims, quotes, or citations, and I will treat any source like output as something that must be verified independently. When evaluating new tools, I will ask whether they make verification easy, how they handle uncertainty, what the interface nudges me to skip, and how they surface or hide perspective and bias.</p>
        </section>
    </main>

    <footer>
        <p>&copy; 2026 Hadley Arcemont | <a href="https://github.com/harcemont/dcda-portfolio">GitHub</a></p>
    </footer>
</body>
</html>